{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Name :- Devashish Mayur Potnis\n",
        "#Roll No :- 43557\n",
        "#Practical No :- 8"
      ],
      "metadata": {
        "id": "FEpZTMW3shIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import re    # for regular expressions\n",
        "import nltk  # for text manipulation\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "wrTn_B87mBW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting display options and ignoring warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZWYdHmtt_Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the datasets\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test_tweets_anuFYb8.csv')\n",
        "\n"
      ],
      "metadata": {
        "id": "h-hYMF97uEJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying sample data\n",
        "print(train[train['label'] == 0].head())\n",
        "print('--------------------------------------')\n",
        "print(train[train['label'] == 1].head())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWWQJ_CCuFcd",
        "outputId": "28d74aa6-6b34-499a-eaa5-304f1e831ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id  label  \\\n",
            "0   1      0   \n",
            "1   2      0   \n",
            "2   3      0   \n",
            "3   4      0   \n",
            "4   5      0   \n",
            "\n",
            "                                                                                                                        tweet  \n",
            "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
            "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
            "2                                                                                                         bihday your majesty  \n",
            "3                                      #model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦    \n",
            "4                                                                                      factsguide: society now    #motivation  \n",
            "--------------------------------------\n",
            "    id  label  \\\n",
            "13  14      1   \n",
            "14  15      1   \n",
            "17  18      1   \n",
            "23  24      1   \n",
            "34  35      1   \n",
            "\n",
            "                                                                                                       tweet  \n",
            "13                                @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot    \n",
            "14     no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins  \n",
            "17                                                                                    retweet if you agree!   \n",
            "23                                                           @user @user lumpy says i am a . prove it lumpy.  \n",
            "34  it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining train and test datasets for preprocessing\n",
        "combi = pd.concat([train, test], ignore_index=True)  # ✅ Corrected\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WpJf_w1vuG0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove unwanted patterns from the tweets\n",
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt\n",
        "\n"
      ],
      "metadata": {
        "id": "JEt8GtjTuIMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Twitter handles (@user)\n",
        "combi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")\n"
      ],
      "metadata": {
        "id": "Tr51WSu3uJV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Removing special characters, numbers, punctuations\n",
        "combi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "\n"
      ],
      "metadata": {
        "id": "aOFxbCrjuLQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing short words (length <= 3)\n",
        "combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "\n"
      ],
      "metadata": {
        "id": "yJ5_uUxwuMqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the tweets\n",
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
        "\n"
      ],
      "metadata": {
        "id": "ibOgVKMcuN0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming the tokens\n",
        "stemmer = PorterStemmer()\n",
        "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "\n"
      ],
      "metadata": {
        "id": "xf_QEC-guSKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detokenizing the tokens\n",
        "for i in range(len(tokenized_tweet)):\n",
        "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "combi['tidy_tweet'] = tokenized_tweet\n",
        "\n",
        "# Extracting features using Bag-of-Words\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow = bow_vectorizer.fit_transform(combi['tidy_tweet'])\n",
        "\n"
      ],
      "metadata": {
        "id": "KntY5JIguTwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting features using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\n",
        "\n",
        "# Splitting data into training and validation sets\n",
        "train_bow = bow[:31962, :]\n",
        "test_bow = bow[31962:, :]\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n",
        "\n",
        "train_tfidf = tfidf[:31962, :]\n",
        "test_tfidf = tfidf[31962:, :]\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]\n",
        "\n"
      ],
      "metadata": {
        "id": "ZR7OMSm1uVQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train model and evaluate F1 score\n",
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return f1_score(predictions, yvalid)\n",
        "\n",
        "# Logistic Regression on Bag-of-Words features\n",
        "lreg = LogisticRegression()\n",
        "f1_bow = train_model(lreg, xtrain_bow, ytrain, xvalid_bow)\n",
        "print(f\"F1 Score (Bag-of-Words): {f1_bow}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw2wTKtLuWpO",
        "outputId": "6ee3b492-be48-42f2-e555-0b834a66cf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (Bag-of-Words): 0.48770894788593905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression on TF-IDF features\n",
        "f1_tfidf = train_model(lreg, xtrain_tfidf, ytrain, xvalid_tfidf)\n",
        "print(f\"F1 Score (TF-IDF): {f1_tfidf}\")\n",
        "\n",
        "# Random Forest on Bag-of-Words features\n",
        "rf = RandomForestClassifier(n_estimators=400, random_state=11)\n",
        "f1_rf_bow = train_model(rf, xtrain_bow, ytrain, xvalid_bow)\n",
        "print(f\"F1 Score (Random Forest, Bag-of-Words): {f1_rf_bow}\")\n",
        "\n",
        "# Random Forest on TF-IDF features\n",
        "f1_rf_tfidf = train_model(rf, xtrain_tfidf, ytrain, xvalid_tfidf)\n",
        "print(f\"F1 Score (Random Forest, TF-IDF): {f1_rf_tfidf}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDbXnaeDuYiD",
        "outputId": "248ee65b-c621-4205-a539-af3e1cd26dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (TF-IDF): 0.46282722513089003\n",
            "F1 Score (Random Forest, Bag-of-Words): 0.5205905205905206\n",
            "F1 Score (Random Forest, TF-IDF): 0.5402405180388529\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine on Bag-of-Words features\n",
        "svc = SVC()\n",
        "f1_svc_bow = train_model(svc, xtrain_bow, ytrain, xvalid_bow)\n",
        "print(f\"F1 Score (SVM, Bag-of-Words): {f1_svc_bow}\")\n",
        "\n",
        "# Support Vector Machine on TF-IDF features\n",
        "f1_svc_tfidf = train_model(svc, xtrain_tfidf, ytrain, xvalid_tfidf)\n",
        "print(f\"F1 Score (SVM, TF-IDF): {f1_svc_tfidf}\")\n",
        "\n",
        "# XGBoost on Bag-of-Words features\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=11)\n",
        "f1_xgb_bow = train_model(xgb, xtrain_bow, ytrain, xvalid_bow)\n",
        "print(f\"F1 Score (XGBoost, Bag-of-Words): {f1_xgb_bow}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrT_lhmuaBH",
        "outputId": "a1b04337-e357-4c0c-8e66-678d93bac1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (SVM, Bag-of-Words): 0.46680497925311204\n",
            "F1 Score (SVM, TF-IDF): 0.48925281473899696\n",
            "F1 Score (XGBoost, Bag-of-Words): 0.46502057613168724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost on TF-IDF features\n",
        "f1_xgb_tfidf = train_model(xgb, xtrain_tfidf, ytrain, xvalid_tfidf)\n",
        "print(f\"F1 Score (XGBoost, TF-IDF): {f1_xgb_tfidf}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtDIWiluub0h",
        "outputId": "7303e9d4-df72-488c-c522-3b765941f84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score (XGBoost, TF-IDF): 0.4564994882292733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Word2Vec embeddings\n",
        "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\n",
        "model_w2v = Word2Vec(tokenized_tweet, vector_size=200, window=5, min_count=2, sg=1, hs=0, negative=10, workers=2, seed=34)\n",
        "model_w2v.train(tokenized_tweet, total_examples=len(combi['tidy_tweet']), epochs=20)\n",
        "\n",
        "# Function to compute average Word2Vec for each tweet\n",
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size)\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        if word in model_w2v.wv:\n",
        "            vec += model_w2v.wv[word]\n",
        "            count += 1\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS32vb87uczh",
        "outputId": "2c088467-a2d1-488f-d14f-76697f0295d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    }
  ]
}